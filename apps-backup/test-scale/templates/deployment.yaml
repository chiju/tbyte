apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-scale
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-scale
  template:
    metadata:
      labels:
        app: test-scale
    spec:
      containers:
      - name: stress
        image: polinux/stress
        command: ["stress"]
        args: ["--cpu", "1", "--vm", "1", "--vm-bytes", "50M"]
        resources:
          requests:
            cpu: 200m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: test-scale
  namespace: default
spec:
  scaleTargetRef:
    name: test-scale
  minReplicaCount: 1
  maxReplicaCount: 2
  triggers:
  - type: cpu
    metricType: Utilization
    metadata:
      value: "50"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-scale-info
  namespace: default
data:
  README.md: |
    # KEDA + Karpenter Scale Test
    
    This tests both KEDA (pod autoscaling) and Karpenter (node autoscaling):
    
    1. Deployment starts with 1 replica
    2. KEDA monitors CPU usage (target: 50%)
    3. High CPU load triggers KEDA to scale pods (up to 20)
    4. Pending pods trigger Karpenter to provision Spot nodes
    5. When load decreases, KEDA scales down pods
    6. Karpenter consolidates and removes unused nodes
    
    Watch:
    - Pods: kubectl get pods -n default -l app=test-scale -w
    - HPA: kubectl get hpa -n default
    - Nodes: kubectl get nodes -w
