# EKS GitOps Lab

Production-ready Amazon EKS infrastructure with GitOps using ArgoCD, fully automated via GitHub Actions and Terraform.

## ğŸš€ From Scratch to Production

This project demonstrates a **complete GitOps workflow** from zero to a fully automated Kubernetes cluster:

1. **Bootstrap** â†’ Create S3 backend for Terraform state
2. **Setup** â†’ Configure IAM role with OIDC authentication
3. **Deploy** â†’ Push to GitHub, infrastructure deploys automatically
4. **GitOps** â†’ ArgoCD syncs applications from Git every 30 seconds
5. **Scale** â†’ Karpenter autoscales nodes, KEDA autoscales pods
6. **Monitor** â†’ Prometheus + Grafana for metrics, Loki for logs
7. **Cleanup** â†’ One command destroys everything

**Total setup time:** ~20 minutes (mostly waiting for EKS cluster)

**Manual steps:** Only 3 (bootstrap, OIDC, GitHub App)

**Everything else:** Fully automated via GitHub Actions and ArgoCD

## ğŸ¯ What Gets Deployed

### Infrastructure
- **EKS Cluster**: Kubernetes 1.34 with managed node groups (2 t3.medium nodes)
- **Networking**: VPC with public/private subnets across 2 AZs
- **Storage**: EBS-backed persistent volumes
- **Autoscaling**: Karpenter for intelligent node scaling

### GitOps & Automation
- **ArgoCD**: Automated application deployment with app-of-apps pattern
- **GitHub Actions**: OIDC-based CI/CD pipeline
- **Terraform**: Infrastructure as Code with S3 remote state

### Applications & Services
- **nginx**: Web server with KEDA autoscaling
- **KEDA**: Event-driven pod autoscaling (CPU/Memory triggers)
- **Karpenter**: Intelligent node autoscaling and bin-packing
- **Prometheus Stack**: Metrics collection with **persistent storage** (15 days retention)
- **Grafana**: Metrics visualization with CloudWatch integration and **persistent dashboards**
- **Loki**: Log aggregation backend
- **Promtail**: Log collection from all pods
- **Event Exporter**: Kubernetes events to Loki for Grafana visualization

### Secrets Management
- **HashiCorp Vault**: Centralized secrets management with audit logging
- **Secrets Store CSI Driver**: Kubernetes-native secret injection (no sidecars!)
- **Vault CSI Provider**: Direct integration between Vault and Kubernetes pods
- **Demo Apps**: Working examples showing Vault integration patterns

### AWS Controllers for Kubernetes (ACK)
- **ACK EKS Controller**: Manages EKS resources via Kubernetes CRDs
- **Access Entries**: Automatically created from SSO roles
- **GitOps-native**: Self-healing access management

## ğŸ” Security Features

### Authentication & Authorization
- âœ… **AWS OIDC**: No stored credentials
- âœ… **Federated Authentication**: GitHub Actions authenticates via OIDC
- âœ… **IAM Identity Center**: SSO with multiple users and permission sets
- âœ… **ACK EKS Controller**: Automatic AccessEntry creation from SSO roles
- âœ… **RBAC**: Role-based access control with namespace isolation
- âœ… **IAM Roles**: Least privilege access for all services
- âœ… **IRSA**: IAM Roles for Service Accounts (Karpenter, Grafana)
- âœ… **Encrypted State**: S3 backend with encryption at rest

### Data Protection
- âœ… **No Secrets in Code**: All sensitive data in GitHub Secrets
- âœ… **Branch Protection**: PRs required via workflow concurrency
- âœ… **State Locking**: Native S3 locking prevents concurrent modifications

### Security Scanning
- âœ… **Checkov**: IaC security scanning in CI/CD pipeline
- âœ… **Terraform Validation**: Format and validation checks
- â„¹ï¸ **Note**: Checkov chosen for deep Terraform analysis

## ğŸ“‹ Prerequisites

- AWS CLI configured (`aws configure`)
- GitHub CLI (`gh auth login`)
- Terraform (v1.13.5+)
- kubectl
- Git

## ğŸš€ Quick Start (3 Steps)

### 1. Bootstrap Backend

```bash
./scripts/bootstrap-backend.sh
```

**What it does:**
- Creates S3 bucket for Terraform state (with versioning & encryption)
- Uses native S3 locking (no DynamoDB needed)
- **Automatically updates** `terraform/backend.tf` with bucket name

**Output:** 
```
âœ… Backend created successfully!
âœ… Updated terraform/backend.tf automatically!
```

### 2. Setup OIDC Access

```bash
./scripts/setup-oidc-access.sh
```

**What it does:**
- Creates GitHub OIDC provider in AWS (if not exists)
- Creates IAM role for GitHub Actions
- Configures federated credentials
- **Automatically adds 3 GitHub secrets**

**Output:**
```
âœ… OIDC setup complete!
âœ… GitHub secrets added!
```

### 3. Create GitHub App (One-time Setup)

**If you don't have a GitHub App yet, create one:**

**Go to:** https://github.com/settings/apps/new

**Required Settings:**
- **Name:** `ArgoCD-EKS-GitOps` (or any name)
- **Homepage:** `https://github.com/YOUR_USERNAME/eks-gitops-lab`
- **Webhook:** âœ… **Uncheck "Active"** (we don't need webhooks)
- **Repository permissions:**
  - **Contents:** `Read-only` (ArgoCD needs to read your repo)
  - **Metadata:** `Read-only` (automatically required)
- **Where can this app be installed:** `Only on this account`

**After creation:**
1. **Generate private key** â†’ Downloads `.pem` file
2. **Note App ID** â†’ Shown on the app page
3. **Install app** â†’ Click "Install App" â†’ Select `eks-gitops-lab` repository
4. **Note Installation ID** â†’ From URL: `github.com/settings/installations/XXXXXXXX`

**Store GitHub App secrets:**
```bash
cd ~/Downloads
gh secret set ARGOCD_APP_PRIVATE_KEY < argocd-eks-gitops.*.private-key.pem
gh secret set ARGOCD_APP_ID -b "YOUR_APP_ID"
gh secret set ARGOCD_APP_INSTALLATION_ID -b "YOUR_INSTALLATION_ID"
```

**âœ… GitHub App configured! This is reusable for future deployments.**

### 4. Deploy

```bash
git add .
git commit -m "Initial deployment"
git push origin main
```

**That's it!** GitHub Actions will:
1. Run terraform plan (security scan)
2. Deploy EKS cluster (~15 minutes)
3. Install ArgoCD
4. Update app configs with cluster info
5. Deploy all applications automatically

## ğŸ—ï¸ Architecture

### Infrastructure Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AWS Cloud                           â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    VPC (10.0.0.0/16)                  â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚ â”‚
â”‚  â”‚  â”‚  Public Subnet   â”‚      â”‚  Public Subnet   â”‚     â”‚ â”‚
â”‚  â”‚  â”‚  10.0.1.0/24     â”‚      â”‚  10.0.2.0/24     â”‚     â”‚ â”‚
â”‚  â”‚  â”‚  (AZ-1)          â”‚      â”‚  (AZ-2)          â”‚     â”‚ â”‚
â”‚  â”‚  â”‚  - NAT Gateway   â”‚      â”‚                  â”‚     â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”‚
â”‚  â”‚           â”‚                         â”‚                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚ â”‚
â”‚  â”‚  â”‚ Private Subnet   â”‚      â”‚ Private Subnet   â”‚     â”‚ â”‚
â”‚  â”‚  â”‚ 10.0.37.0/24     â”‚      â”‚ 10.0.60.0/24     â”‚     â”‚ â”‚
â”‚  â”‚  â”‚ (AZ-1)           â”‚      â”‚ (AZ-2)           â”‚     â”‚ â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”‚
â”‚  â”‚  â”‚ â”‚ EKS Nodes    â”‚ â”‚      â”‚ â”‚ EKS Nodes    â”‚ â”‚     â”‚ â”‚
â”‚  â”‚  â”‚ â”‚ t3.medium    â”‚ â”‚      â”‚ â”‚ t3.medium    â”‚ â”‚     â”‚ â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GitOps Flow

```
Developer â†’ PR â†’ Plan â†’ Review â†’ Merge â†’ Apply â†’ Update Configs â†’ ArgoCD Syncs
```

### Application Deployment

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ArgoCD                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  core-apps (App of Apps)                                    â”‚
â”‚  â”œâ”€ Monitors: argocd-apps/ directory                       â”‚
â”‚  â”œâ”€ Auto-sync: Every 30 seconds                            â”‚
â”‚  â””â”€ Auto-prune: Removes deleted apps                       â”‚
â”‚                                                              â”‚
â”‚  Applications                                                â”‚
â”‚  â”œâ”€ nginx (with KEDA autoscaling)                          â”‚
â”‚  â”œâ”€ keda (pod autoscaling controller)                      â”‚
â”‚  â”œâ”€ karpenter (node autoscaling)                           â”‚
â”‚  â”œâ”€ kube-prometheus-stack (monitoring)                     â”‚
â”‚  â”œâ”€ loki (log aggregation)                                 â”‚
â”‚  â””â”€ promtail (log collection)                              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ terraform.yml           # Main CI/CD pipeline
â”‚   â”œâ”€â”€ terraform-destroy.yml   # Infrastructure cleanup
â”‚   â””â”€â”€ update-app-values.yml   # Update configs from Terraform
â”œâ”€â”€ apps/                       # Helm charts for applications
â”‚   â”œâ”€â”€ nginx/
â”‚   â”œâ”€â”€ keda/
â”‚   â”œâ”€â”€ karpenter/
â”‚   â”œâ”€â”€ kube-prometheus-stack/
â”‚   â”œâ”€â”€ loki/
â”‚   â”œâ”€â”€ promtail/
â”‚   â”œâ”€â”€ event-exporter/        # Kubernetes events to Loki
â”‚   â”œâ”€â”€ secrets-store-csi/     # CSI driver for secrets
â”‚   â”œâ”€â”€ vault/                 # HashiCorp Vault
â”‚   â”œâ”€â”€ vault-demo/            # Vault integration demo
â”‚   â”œâ”€â”€ myapp/                 # Example app with Vault
â”‚   â”œâ”€â”€ ack-eks-controller/    # ACK EKS controller
â”‚   â”œâ”€â”€ access-entries/        # EKS access entries via ACK
â”‚   â””â”€â”€ rbac-setup/            # RBAC roles and bindings
â”œâ”€â”€ argocd-apps/               # ArgoCD application definitions
â”‚   â”œâ”€â”€ nginx.yaml
â”‚   â”œâ”€â”€ keda.yaml
â”‚   â”œâ”€â”€ karpenter.yaml
â”‚   â”œâ”€â”€ kube-prometheus-stack.yaml
â”‚   â”œâ”€â”€ loki.yaml
â”‚   â”œâ”€â”€ promtail.yaml
â”‚   â”œâ”€â”€ event-exporter.yaml
â”‚   â”œâ”€â”€ ack-eks-controller.yaml
â”‚   â”œâ”€â”€ access-entries.yaml
â”‚   â””â”€â”€ rbac-setup.yaml
â”œâ”€â”€ terraform/                 # Terraform infrastructure
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ aks/              # EKS cluster configuration
â”‚   â”‚   â”œâ”€â”€ argocd/           # ArgoCD Helm deployment
â”‚   â”‚   â””â”€â”€ vpc/              # Virtual network
â”‚   â”œâ”€â”€ backend.tf            # Terraform backend configuration
â”‚   â”œâ”€â”€ main.tf               # Main Terraform configuration
â”‚   â”œâ”€â”€ variables.tf          # Variable definitions
â”‚   â”œâ”€â”€ outputs.tf            # Output definitions
â”‚   â””â”€â”€ provider.tf           # Provider configuration
â”œâ”€â”€ scripts/                   # Automation scripts
â”‚   â”œâ”€â”€ bootstrap-backend.sh
â”‚   â”œâ”€â”€ setup-oidc-access.sh
â”‚   â””â”€â”€ cleanup-all.sh
â””â”€â”€ README.md
```

## ğŸ® Accessing Services

### EKS Cluster

```bash
# Get credentials
aws eks update-kubeconfig --name eks-gitops-lab --region eu-central-1

# Check cluster
kubectl get nodes
kubectl get pods --all-namespaces
```

### ArgoCD UI

```bash
# Port forward
kubectl port-forward svc/argocd-server -n argocd 8080:443

# Get password
kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 -d

# Open browser
open https://localhost:8080
# Username: admin
# Password: (from above command)
```

### Grafana

```bash
# Port forward
kubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 3000:80

# Get password
kubectl get secret kube-prometheus-stack-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 -d

# Open browser
open http://localhost:3000
# Username: admin
# Password: (from above command)
```

### Prometheus

```bash
kubectl port-forward svc/kube-prometheus-stack-prometheus -n monitoring 9090:9090
open http://localhost:9090
```

### AWS SSO Access

```bash
# Configure SSO profile
aws configure sso
# SSO start URL: https://d-99675f4fc7.awsapps.com/start
# SSO Region: eu-central-1
# Account: 432801802107
# Role: EKSDeveloper / EKSDevOps / EKSReadOnly

# Login
aws sso login --profile <profile-name>

# Access EKS
aws eks update-kubeconfig --name eks-gitops-lab --region eu-central-1 --profile <profile-name>
kubectl get pods -n dev  # Developer access
kubectl get nodes        # DevOps access
```

**User Roles:**
- **EKSDeveloper**: Full access to `dev` namespace only
- **EKSDevOps**: Full cluster access (all namespaces, nodes)
- **EKSReadOnly**: Read-only access to all namespaces

## ğŸ§¹ Cleanup

### Complete Cleanup

```bash
./scripts/cleanup-all.sh
```

This removes:
- âœ… IAM role
- âœ… S3 bucket and all objects
- âœ… GitHub secrets
- âœ… Local Terraform state files

### Partial Cleanup (Keep Backend)

```bash
# Destroy infrastructure only (manual trigger required)
gh workflow run terraform-destroy.yml -f confirm=destroy
```

## ğŸ› Troubleshooting

### Issue: Workflow fails with permission error

**Solution:** The IAM role needs proper permissions. Check:
```bash
aws iam get-role --role-name GitHubActionsEKSRole
```

### Issue: ArgoCD not syncing apps

**Possible causes:**
1. GitHub token expired
2. Repository URL incorrect
3. Branch name mismatch

**Solution:**
```bash
# Check ArgoCD repo secret
kubectl get secret argocd-repo -n argocd -o yaml

# Update if needed
kubectl delete secret argocd-repo -n argocd
# Re-run update-app-values workflow
gh workflow run update-app-values.yml
```

### Issue: Karpenter not scaling nodes

**Solution:** Check if Karpenter has correct cluster info:
```bash
# Manually trigger update workflow
gh workflow run update-app-values.yml

# Verify Karpenter config
kubectl get ec2nodeclass -o yaml
```

### Issue: Pods pending due to insufficient resources

**Solution:** Karpenter will automatically provision nodes. Check:
```bash
# Check Karpenter logs
kubectl logs -n karpenter -l app.kubernetes.io/name=karpenter

# Check pending pods
kubectl get pods --all-namespaces --field-selector=status.phase=Pending
```

## ğŸ“Š Monitoring & Observability

### Metrics (Prometheus + Grafana)

- **Node metrics**: CPU, memory, disk, network
- **Pod metrics**: Resource usage per pod
- **Cluster metrics**: Overall cluster health
- **CloudWatch integration**: Grafana can query CloudWatch

### Logs (Loki + Promtail)

- **Centralized logging**: All pod logs in one place
- **Query language**: LogQL for powerful log queries
- **Retention**: Configurable log retention policies
- **Integration**: Grafana dashboards for log visualization

### Kubernetes Events (Event Exporter)

- **Event collection**: All K8s events sent to Loki
- **Grafana visualization**: View events in Grafana Explore
- **Query**: `{app="event-exporter"}` or `{type="Warning"}`
- **Filtering**: By namespace, reason, type, kind, name
- **Pod metrics**: Resource usage per pod
- **Cluster metrics**: Overall cluster health
- **CloudWatch integration**: Grafana can query CloudWatch

### Logs (Loki + Promtail)

- **Centralized logging**: All pod logs in one place
- **Query language**: LogQL for powerful log queries
- **Retention**: Configurable log retention policies
- **Integration**: Grafana dashboards for log visualization

### Autoscaling

**KEDA (Pod Autoscaling):**
- CPU-based: Scale on CPU utilization
- Memory-based: Scale on memory usage
- Custom metrics: Scale on any Prometheus metric

**Karpenter (Node Autoscaling):**
- Intelligent provisioning: Right-sized nodes
- Bin-packing: Efficient resource utilization
- Fast scaling: Nodes ready in ~2 minutes
- Cost optimization: Spot instances support

## ğŸ’° Cost Optimization

### Current Setup (2 nodes)

- **EKS Control Plane**: ~$73/month
- **EC2**: 2 x t3.medium (~$60/month)
- **NAT Gateway**: ~$32/month
- **EBS Volumes**: ~$10/month
- **Total**: ~$175/month

### Cost Saving Tips

1. **Use Karpenter with Spot** - Save up to 90% on compute
2. **Scale down** when not in use
3. **Use smaller node sizes** for dev/test
4. **Destroy infrastructure** when not needed

```bash
# Destroy when not in use
gh workflow run terraform-destroy.yml -f confirm=destroy

# Redeploy when needed
git commit --allow-empty -m "Redeploy" && git push
```

## ğŸ”’ Security Best Practices

### Implemented

- âœ… No credentials in code or version control
- âœ… Federated authentication (OIDC)
- âœ… Encrypted Terraform state
- âœ… IAM roles with least privilege
- âœ… IRSA for pod-level permissions
- âœ… Secrets stored in GitHub Secrets
- âœ… Workflow concurrency control

### Recommended for Production

**Security Enhancements:**
- ğŸ”² **External Secrets Operator** - Sync secrets from AWS Secrets Manager
- ğŸ”² **Private Cluster Endpoint** - Restrict API server access
- ğŸ”² **Network Policies** - Control pod-to-pod traffic
- ğŸ”² **Pod Security Standards** - Enforce security policies
- ğŸ”² **AWS Config** - Compliance and governance
- ğŸ”² **KMS Encryption** - Encrypt Kubernetes secrets at rest

**Infrastructure Improvements:**
- ğŸ”² **Separate Node Groups** - System vs user workloads
- ğŸ”² **Production Instance Types** - t3.large or larger
- ğŸ”² **Resource Limits** - CPU/memory limits on all pods
- ğŸ”² **Velero Backups** - Disaster recovery
- ğŸ”² **Multi-region** - High availability

**Operational:**
- ğŸ”² **Cost Alerts** - AWS Budgets and alerts
- ğŸ”² **Terraform Workspaces** - Dev/staging/prod environments
- ğŸ”² **Runbooks** - Incident response procedures
- ğŸ”² **SLO/SLA Monitoring** - Service level objectives

## ğŸ“š What's Automated

- âœ… S3 backend creation
- âœ… Backend configuration auto-update
- âœ… IAM role creation and configuration
- âœ… OIDC provider setup
- âœ… GitHub secrets (3 of 5 automated)
- âœ… EKS cluster deployment
- âœ… ArgoCD installation and configuration
- âœ… Application deployment via GitOps
- âœ… Karpenter configuration with cluster info
- âœ… Grafana CloudWatch integration
- âœ… KEDA autoscaling setup
- âœ… Monitoring stack deployment

## âœ‹ What's Manual

- âŒ Add `GIT_USERNAME` secret (one-time)
- âŒ Add `ARGOCD_GITHUB_TOKEN` secret (one-time)

## ğŸ” Using Vault for Secrets Management

### Overview

This lab includes **HashiCorp Vault** with **CSI driver integration** - the production-standard pattern for secrets management in Kubernetes.

**Why Vault + CSI?**
- âœ… Secrets never stored in Kubernetes (bypasses etcd completely)
- âœ… No sidecar containers (CSI driver is shared across all pods)
- âœ… Automatic secret rotation without pod restarts
- âœ… Full audit trail of secret access
- âœ… Works with any programming language (just read files)

### Architecture

```
Pod starts
    â†“
Kubernetes mounts CSI volume
    â†“
CSI Driver authenticates with Vault (using ServiceAccount token)
    â†“
Vault validates and returns secrets
    â†“
Secrets appear as files in /mnt/secrets/
    â†“
App reads secrets like normal files
```

### Quick Start

**1. Check Vault is running:**
```bash
kubectl get pods -n vault
# vault-0                                 1/1     Running
# vault-csi-provider-xxxxx                2/2     Running
```

**2. See demo app using Vault:**
```bash
kubectl get pods -n demo
kubectl logs -n demo -l app=demo-app
```

**3. Check example production app:**
```bash
kubectl get pods -n production
kubectl logs -n production -l app=myapp
```

### Adding Secrets to Your App

**Step 1: Create secret in Vault**
```bash
kubectl exec -n vault vault-0 -- vault kv put secret/myapp/prod \
  api_key=your-secret-key \
  db_password=your-db-password
```

**Step 2: Create policy**
```bash
kubectl exec -n vault vault-0 -- sh -c 'vault policy write myapp-prod - <<EOF
path "secret/data/myapp/prod" {
  capabilities = ["read"]
}
EOF'
```

**Step 3: Create Kubernetes role**
```bash
kubectl exec -n vault vault-0 -- vault write auth/kubernetes/role/myapp-prod \
  bound_service_account_names=myapp \
  bound_service_account_namespaces=production \
  policies=myapp-prod \
  ttl=24h
```

**Step 4: Use in your app**
```yaml
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: myapp-secrets
spec:
  provider: vault
  parameters:
    vaultAddress: "http://vault.vault:8200"
    roleName: "myapp-prod"
    objects: |
      - objectName: "api_key"
        secretPath: "secret/data/myapp/prod"
        secretKey: "api_key"
---
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      serviceAccountName: myapp
      containers:
      - name: app
        volumeMounts:
        - name: secrets
          mountPath: /mnt/secrets
          readOnly: true
        env:
        - name: API_KEY
          value: "$(cat /mnt/secrets/api_key)"
      volumes:
      - name: secrets
        csi:
          driver: secrets-store.csi.k8s.io
          volumeAttributes:
            secretProviderClass: "myapp-secrets"
```

### Complete Example

See `apps/myapp/` for a complete working example with:
- Automated Vault configuration (Job)
- SecretProviderClass definition
- Deployment using CSI-mounted secrets
- ArgoCD integration with sync waves

**To deploy your own app:**
1. Copy `apps/myapp/` folder
2. Update secret paths and values in `templates/vault-config.yaml`
3. Update container image in `templates/app.yaml`
4. Create ArgoCD app in `argocd-apps/`
5. Push to Git - ArgoCD deploys automatically!

### Key Benefits

| Feature | Kubernetes Secrets | Vault + CSI |
|---------|-------------------|-------------|
| Storage | etcd (base64) | Vault (encrypted) |
| Access Control | RBAC only | Policy-based + RBAC |
| Audit Trail | None | Full audit log |
| Rotation | Manual pod restart | Automatic |
| Overhead | None | Shared DaemonSet |
| Multi-cloud | No | Yes |

### Production Considerations

**Current Setup (Dev Mode):**
- âš ï¸ In-memory storage (data lost on restart)
- âš ï¸ Single instance (no HA)
- âš ï¸ Root token "root" (insecure)
- âš ï¸ Auto-unsealed (convenient but insecure)

**For Production:**
- âœ… Persistent storage (EBS or S3)
- âœ… HA with 3+ replicas and Raft consensus
- âœ… Auto-unseal with AWS KMS
- âœ… Proper initialization with key sharding
- âœ… Audit logging to CloudWatch
- âœ… Backup and disaster recovery

## ğŸ“ Learning Resources

- [Amazon EKS Documentation](https://docs.aws.amazon.com/eks/)
- [ArgoCD Documentation](https://argo-cd.readthedocs.io/)
- [Karpenter Documentation](https://karpenter.sh/)
- [KEDA Documentation](https://keda.sh/)
- [HashiCorp Vault Documentation](https://developer.hashicorp.com/vault/docs)
- [Secrets Store CSI Driver](https://secrets-store-csi-driver.sigs.k8s.io/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [GitOps Principles](https://opengitops.dev/)

## ğŸ“ License

MIT

## ğŸ¤ Contributing

This is a learning lab project. Feel free to fork and adapt for your needs!

## âš ï¸ Important Notes

### Current Setup
- **Purpose**: Learning and portfolio demonstration
- **Environment**: Lab/Development
- **Instance Type**: t3.medium (cost-optimized)
- **Security**: Basic (OIDC, IRSA, encrypted state)

### For Production Use
This setup provides a **solid foundation** but requires these enhancements:

**Must Have:**
- Private cluster endpoint
- Network policies
- Resource limits on all pods
- External Secrets Operator with AWS Secrets Manager
- Velero backups
- Production instance types (t3.large+)
- KMS encryption for Kubernetes secrets

**Should Have:**
- Separate node groups (system/user)
- Cost alerts and budgets
- Multi-environment setup (dev/staging/prod)
- Comprehensive monitoring and alerting
- Disaster recovery plan

**Cost Considerations:**
- Current setup: ~$175/month
- Production setup: ~$400-600/month (with redundancy)
- Remember to destroy resources when not in use
