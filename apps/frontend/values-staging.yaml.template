# STAGING ENVIRONMENT - Stricter Thresholds
image:
  repository: 432801802107.dkr.ecr.eu-central-1.amazonaws.com/tbyte-staging-frontend
  tag: "latest"
  pullPolicy: Always

service:
  type: ClusterIP
  port: 80

canary:
  enabled: true
  # Stricter Analysis for Staging
  analysis:
    enabled: true
    template: success-rate
    startingStep: 1  # Start analysis earlier (after 5% traffic)
  # Stricter Thresholds for Staging
  thresholds:
    successRate: 0.99      # 99% success rate (stricter than dev)
    latencyP95: 0.3        # 300ms P95 latency (stricter than dev)
    errorRate: 0.005       # 0.5% error rate (stricter than dev)
    cpuUsage: 0.7          # 70% CPU usage (stricter than dev)
    memoryUsage: 80000000  # 80MB memory (stricter than dev)
  # Slower Canary Progression for Staging
  steps:
    - setWeight: 5         # Start with 5% (safer)
    - pause: 60s           # Longer pauses
    - setWeight: 10
    - pause: 120s          # Analysis runs here
    - setWeight: 25
    - pause: 120s
    - setWeight: 50
    - pause: 60s
    - setWeight: 75
    - pause: 60s

istio:
  enabled: true
  virtualService: staging-routes
  namespace: istio-system

# Stricter Rollback Settings for Staging
rollback:
  enabled: true
  failureLimit: 2          # Rollback faster (2 failures vs 3 in dev)
  abortScaleDownDelay: 15  # Faster cleanup
  progressDeadline: 900    # 15 minutes max (longer for safety)

resources:
  requests:
    memory: "128Mi"        # More resources for staging
    cpu: "100m"
  limits:
    memory: "256Mi" 
    cpu: "200m"

replicas: 5                # More replicas for staging

# Staging Monitoring
monitoring:
  prometheus:
    enabled: true
    address: "http://kube-prometheus-stack-prometheus.monitoring:9090"
  alerts:
    enabled: true
    webhook: ""  # Add staging alerts webhook
