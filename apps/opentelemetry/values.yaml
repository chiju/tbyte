# OpenTelemetry Operator Configuration
opentelemetry-operator:
  enabled: true
  admissionWebhooks:
    certManager:
      enabled: false
    autoGenerateCert:
      enabled: true
  manager:
    image:
      repository: ghcr.io/open-telemetry/opentelemetry-operator/opentelemetry-operator
      tag: "0.139.0"
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 64Mi

# OpenTelemetry Collector Configuration
opentelemetry-collector:
  enabled: true
  mode: daemonset
  image:
    repository: otel/opentelemetry-collector-contrib
    tag: "0.115.0"
  
  # Collector configuration
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      
      # Kubernetes metrics
      k8s_cluster:
        auth_type: serviceAccount
        node_conditions_to_report: [Ready, MemoryPressure, DiskPressure, PIDPressure]
        allocatable_types_to_report: [cpu, memory, storage]
      
      # Host metrics
      hostmetrics:
        collection_interval: 10s
        scrapers:
          cpu: {}
          disk: {}
          load: {}
          filesystem: {}
          memory: {}
          network: {}
          process: {}
      
      # Prometheus metrics from our monitoring stack
      prometheus:
        config:
          scrape_configs:
            - job_name: 'kubernetes-pods'
              kubernetes_sd_configs:
                - role: pod
              relabel_configs:
                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                  action: keep
                  regex: true
                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                  action: replace
                  target_label: __metrics_path__
                  regex: (.+)

    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
      
      # Add Kubernetes attributes
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        filter:
          node_from_env_var: KUBE_NODE_NAME
        extract:
          metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: connection

    exporters:
      # Export to Prometheus (our existing monitoring)
      prometheus:
        endpoint: "0.0.0.0:8889"
        
      # Export to our existing Loki for logs
      loki:
        endpoint: "http://loki-gateway.loki.svc.cluster.local/loki/api/v1/push"
        
      # Debug exporter for troubleshooting
      debug:
        verbosity: detailed

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [k8sattributes, batch]
          exporters: [debug]
        
        metrics:
          receivers: [otlp, k8s_cluster, hostmetrics, prometheus]
          processors: [k8sattributes, batch]
          exporters: [prometheus, debug]
        
        logs:
          receivers: [otlp]
          processors: [k8sattributes, batch]
          exporters: [loki, debug]

  # Resources
  resources:
    limits:
      cpu: 256m
      memory: 512Mi
    requests:
      cpu: 256m
      memory: 512Mi

  # Service configuration
  service:
    type: ClusterIP
    
  # Ports configuration
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    prometheus:
      enabled: true
      containerPort: 8889
      servicePort: 8889
      protocol: TCP

  # Tolerations for daemonset to run on all nodes
  tolerations:
    - operator: Exists
      effect: NoSchedule
